# หลักการทำงานและส่วนประกอบ
ในส่วนของบริการที่ใช้งานในโปรเจคนี้จะประกอบไปด้วย 3 ส่วนหลักๆ คือ ส่วนการจัดเก็บข้อมูล ส่วนการประมวลผล และส่วนของการวิเคราะห์ข้อมูลและแสดงผล ซึ่งเป็นการออกแบบที่สามารถทำงานร่วมกันได้ ทำให้สะดวกในการจัดการและเพิ่มประสิทธิภาพในการใช้ทรัพยากรได้เต็มศักยภาพมากยิ่งขึ้น โดยแต่ละส่วนนั้นมีเครื่องมือที่ถูกเรียกใช้งานตามลักษณะงานที่แตกต่างกันออกไปอีกจำนวนหนึ่ง อันได้แก่

## การจัดเก็บข้อมูล (Storage)
Azure มีบริการหลากหลายที่สามารถใช้เก็บข้อมูลขนาดใหญ่ได้ สำหรับโปรเจ็กนี้เราแบ่งการใช้บริการเป็นหลักได้เป็นประเภทดังนี้:
- บริการที่มีการจัดการรวมถึง Azure Data Lake Store, Azure Data Warehouse 
- เทคโนโลยีโอเพนซอร์สที่ใช้แพลตฟอร์ม Apache Hadoop ได้แก่ HDFS, HBase, Hive, Pig, Spark, Storm, Oozie, Sqoop และ Kafka เทคโนโลยีเหล่านี้มีอยู่ใน Azure ในบริการ Azure HDInsight

### Azure Data Lake Store
Azure Data Lake Store เป็นที่เก็บข้อมูลขนาดใหญ่สำหรับองค์กร สำหรับข้อมูลขนาดใหญ่สำหรับการวิเคราะห์ข้อมูล Azure Data Lake ช่วยให้ผู้ใช้สามารถจัดเก็บข้อมูลทุกขนาด ทุกชนิดและความเร็วสูงในการส่งผ่านข้อมูลได้ในที่เดียว สำหรับการวิเคราะห์การปฏิบัติงานและการสำรวจข้อมูล สำหรับการใช้งาน Azure Data Lake Store สามารถเข้าถึงได้จาก Hadoop (พร้อมใช้งานกับ HDInsight cluster) โดยใช้ REST APIs ที่เข้ากันได้กับ WebHDFS ที่ได้รับการออกแบบมาเป็นพิเศษเพื่อให้สามารถวิเคราะห์ข้อมูลที่เก็บไว้ และปรับการทำงานสำหรับสถานการณ์การวิเคราะห์ข้อมูลได้ ความสามารถในการจัดการความยืดหยุ่นความน่าเชื่อถือและความพร้อมใช้งานที่จำเป็นสำหรับกรณีการใช้งานระดับองค์กรในโลกแห่งความเป็นจริง

### Data Factory
Azure Data Factory เป็นบริการรวมข้อมูลแบบ Hybrid ที่จะช่วยให้ผู้ใช้งานสามารถสร้าง ตั้งเวลาทำงาน และจัดลำดับขั้นตอนการทำงาน ETL (Extract, Transform and Load) / ELT (Extract, Load and Transform) ได้ทุกที่ไม่ว่าข้อมูลจะอยู่ที่ใดอย่างในระบบ Cloud หรือในเครือข่ายภายในองค์กร ทั้งนี้ ยังมีระบบการรักษาความปลอดภัยของข้อมูลที่เป็นไปตามข้อกำหนดด้านความปลอดภัย

Data Factory เป็นโซลูชันช่วยในการจัดการข้อมูลขนาดใหญ่ ประกอบด้วยการประมวลผลข้อมูลซ้ำ ๆ เป็นกระบวนการแบบเวิร์กโฟลว์ซึ่งจะแปลงข้อมูลต้นฉบับ ย้ายข้อมูลระหว่างหลายแหล่ง และเก็บข้อมูลโหลดข้อมูลที่ผ่านการประมวลผลไปยังที่เก็บข้อมูลแบบวิเคราะห์ หรือส่งผลการค้นหาไปยังรายงานหรือแดชบอร์ด
- ในส่วนของการทำงานบน Platform นี้ สามารถทำงานได้กับ 30 low frequency and 30 high frequency ทั้งบน Cloud และ On-Premise แต่จะไม่ครอบคลุมการทำ Re-Run หรือการเคลื่อนย้ายข้อมูลใดๆ
- ในโปรเจ็กนี้ ข้อมูลสำหรับการดำเนินการประมวลผลแบบแบทช์จะจัดเก็บอยู่ในที่จัดเก็บไฟล์แบบกระจายซึ่งสามารถเก็บไฟล์ขนาดใหญ่จำนวนมากในรูปแบบต่างๆ ประเภทนี้มักเรียกว่า Data Lake ตัวเลือกสำหรับการใช้พื้นที่จัดเก็บข้อมูลนี้ ได้แก่ Azure Data Lake Store 

### Azure Data Catalog
ดาต้าแคตตาล็อกข้อมูลบน Azure คือบริการคลาวด์ที่ช่วยให้ผู้ใช้สามารถค้นพบแหล่งข้อมูลที่ต้องการ และเข้าใจแหล่งข้อมูลที่พวกเขาต้องการ ในขณะเดียวกันแคตตาล็อกข้อมูลช่วยให้องค์กรได้สามารถเข้าถึงข้อมูลที่มีอยู่ได้อย่างมีประสิทธิภาพ ด้วยแคตตาล็อกข้อมูลผู้ใช้ เช่น นักวิเคราะห์นักวิทยาศาสตร์ข้อมูลหรือนักพัฒนาซอฟต์แวร์ สามารถค้นพบทำความเข้าใจและใช้แหล่งข้อมูลได้
ดาต้าแคตตาล็อกข้อมูลประกอบด้วยรูปแบบข้อมูลและคำอธิบายประกอบของ crowdsourcing เป็นศูนย์กลางสำหรับผู้ใช้องค์กรทุกคนที่มีส่วนร่วมในการสร้างความรู้และสร้างชุมชนและวัฒนธรรมการใช้ข้อมูลร่วมกัน ดาต้าแคตตาล็อก ได้รับการออกแบบมาเพื่อแก้ไขปัญหาเหล่านี้และเพื่อช่วยให้องค์กรต่างๆได้รับประโยชน์สูงสุดจากข้อมูลที่มีอยู่ แคตตาล็อกข้อมูลทำให้แหล่งข้อมูล สามารถค้นพบและเข้าใจได้ง่ายโดยผู้ใช้ที่จัดการข้อมูล 

### CosmosDB
Azure Cosmos DB เป็นฐานข้อมูลแบบกระจายทั่วโลก โดยการทำงานของระบบสามารถบริหารจัดการด้วยการเลือกพื้นที่ทึ่ต้องการ Replicate ข้อมูลไป โดยคลิกปุ่ม Azure Cosmos DB ช่วยให้คุณสามารถปรับขนาดและเก็บข้อมูลได้อย่างอิสระและผ่านพื้นที่ทางภูมิศาสตร์ต่างๆของ Azure มีข้อตกลงในการรับประกันความพร้อมใช้งานและพร้อมกับข้อตกลงระดับบริการที่ครอบคลุม (SLA) บางอย่างที่ดีกว่าบริการฐานข้อมูลอื่น ๆ ได้  ผู้ใช้สามารถกระจายข้อมูลของผู้ใช้ไปยังพื้นที่ Azure ได้ทุกหนแห่งช่วยให้ผู้ใช้สามารถนำข้อมูลของผู้ใช้ไปใช้ที่ผู้ใช้ของผู้ใช้เพื่อให้แน่ใจได้ถึงไม่ความล่าช้าที่อาจเกิดขึ้นกับลูกค้าของผู้ใช้ได้น้อยที่สุด และ CosmosDB ใช้แอ็พพลิเคชัน API แบบ multi-home API ของ Azure Cosmos DB แอปพลิเคชันจะทราบที่อยู่ที่ใกล้ที่สุดอยู่เสมอและส่งคำขอไปยังศูนย์ข้อมูลที่ใกล้ที่สุด ทั้งหมดนี้เป็นไปได้โดยไม่มีการเปลี่ยนแปลง config ผู้ใช้ตั้งค่าภูมิภาคการเขียนและเป็นพื้นที่อ่านได้มากเท่าที่ผู้ใช้ต้องการและส่วนที่เหลือจะถูกจัดการให้ผู้ใช้

เมื่อผู้ใช้เพิ่มและนำพื้นที่ออกจากฐานข้อมูล Cosmos DB แอ็พพลิเคชันของผู้ใช้ไม่จำเป็นต้องมีการปรับใช้ใหม่และยังสามารถใช้งานได้อย่างต่อเนื่องเนื่องจากความสามารถในการทำแบบ multi-homing API

## การประมวลผล (Processing Tools)
ในโปรเจ็นนี้เราได้เตรียมระบบสำหรับการประมวลผลขนาดใหญ่ เนื่องจากชุดข้อมูลมีขนาดใหญ่มากซึ่งโดยปกติแล้วโซลูชันข้อมูลขนาดใหญ่จะต้องประมวลผลไฟล์ข้อมูลโดยใช้งานแบทช์ที่ทำงานเป็นช่วงเวลานานเพื่อกรองข้อมูลรวมและเตรียมข้อมูลสำหรับการวิเคราะห์ โดยปกติงานเหล่านี้เกี่ยวข้องกับการอ่านไฟล์ต้นฉบับประมวลผลและเขียนผลลัพธ์ข้อมูลออกไปยังไฟล์ใหม่ ซึ่งเราจะใช้ HIVE, Pig และ Map/Reduce ใน HDInsight Hadoop ซึ่งสามารถใช้โปรแกรมภาษา Java, Scala หรือ Python ในกลุ่ม HDInsight Spark อีกด้วย

Azure จัดเตรียมข้อมูลสำหรับการวิเคราะห์ ทำการประมวลผลให้อยู่ในรูปแบบเชิงโครงสร้างที่เหมาะในการนำเสนอข้อมูลที่เรียกใช้โดยเครื่องมือวิเคราะห์ การเก็บข้อมูลเชิงวิเคราะห์ที่ใช้ในการ query เหล่านี้อาจเป็นในรูปแบบ Data Warehouse แบบ Kimball ตามที่เห็นในโซลูชัน Business Intelligence แบบทั่วไป หรือข้อมูลอาจถูกนำเสนอผ่านเทคโนโลยี NoSQL เช่น HBase หรือฐานข้อมูล Hive แบบโต้ตอบที่ให้ข้อมูลเมตาดาต้า บนไฟล์ข้อมูลในที่จัดเก็บข้อมูลแบบกระจาย อีกบริการหนึ่งคือ Azure SQL Data Warehouse ให้บริการที่มีการจัดการสำหรับคลังข้อมูลขนาดใหญ่บนระบบคลาวด์ และ Azure HDInsight (Hadoop/Spark)สนับสนุน Interactive Hive, HBase และ Spark SQL ซึ่งสามารถนำมาใช้เพื่อให้บริการข้อมูลเพื่อการวิเคราะห์ได้เป็นอย่างดี

## ส่วนของการวิเคราะห์ข้อมูลและแสดงผล
### Machine Learning Experimentation
การวิเคราะห์และการรายงาน เป็นสิ่งสำคัญสำหรับโปรเจ็กนี้ การใช้ Machine Learning ซึ่งเป็นเทคนิคการวิเคราะห์ข้อมูลที่ช่วยให้คอมพิวเตอร์สามารถใช้ข้อมูลที่มีอยู่เพื่อพยากรณ์พฤติกรรมในอนาคตผลและแนวโน้ม และสามารถทำให้แอปและอุปกรณ์ฉลาดขึ้นได้ เช่น การซื้อสินค้าออนไลน์ Machine Learning จะช่วยแนะนำผลิตภัณฑ์อื่น ๆ ที่ลูกค้าอาจต้องการจากสิ่งที่คุณได้ซื้อ เมื่อบัตรเครดิตของคุณถูกรูด Machine Learning จะเปรียบเทียบการทำธุรกรรมกับฐานข้อมูลของธุรกรรมและช่วยในการตรวจสอบการฉ้อโกง หรือใน IoT ต่างๆ 
	Azure Machine Learning สนับสนุนเทคโนโลยีโอเพนซอร์ส ผู้ใช้สามารถใช้แพคเกจ Python แบบโอเพ่นซอร์นับหมื่นชุดได้เช่น scikit Learning, TensorFlow, Microsoft Cognitive Toolkit, Spark ML

สำหรับเป้าหมายของโปรเจ็กนี้คือ ใช้ข้อมูลและวิเคราะห์เพื่อให้ได้ข้อมูลเชิงลึกเกี่ยวกับข้อมูลผ่านการวิเคราะห์และการรายงาน เพื่อช่วยให้ผู้ใช้สามารถวิเคราะห์ข้อมูลสถาปัตยกรรมอาจรวมถึงเลเยอร์การสร้างแบบจำลองข้อมูลเช่น algorithm ต่างๆ Regression, classification, prediction เป็นต้น และระบบยังสามารถสร้าง OLAP ข้อมูลแบบลูกบาศก์หลายมิติหรือแบบจำลองข้อมูลแบบตารางใน Azure Analysis Services นอกจากนี้ยังอาจสนับสนุน BI ที่ผู้ใข้งานสามารถ สร้างรายงานด้วยตนเองโดยใช้เทคโนโลยีการสร้างแบบจำลองและการแสดงภาพใน Microsoft Power BI หรือ Microsoft Excel การวิเคราะห์และการรายงานยังสามารถใช้รูปแบบของการมีปฎิสัมพันกับนักวิทยาศาสตร์ข้อมูลหรือนักวิเคราะห์ข้อมูล และยังสามารถอำนวยความสะดวกให้กับนักวิเคราะห์ ในการใช้งาน Tool ที่เป็นที่นิยมเช่น การสนับสนุนโน้ตบุ๊คการวิเคราะห์เช่น Jupyter ทำให้ผู้ใช้เหล่านี้สามารถใช้ประโยชน์จากทักษะที่มีอยู่กับ Python หรือ R. สำหรับการสำรวจข้อมูลขนาดใหญ่ผู้ใช้สามารถใช้ Microsoft R Server แบบสแตนด์อโลนหรือ บน Spark

### Data Science VM
Data Science VM (DSVM) ที่สร้างขึ้นมาเฉพาะสำหรับการทำการวิเคราะห์ข้อมูลสำหรับนักวิทยาศาสตร์ข้อมูล มีเครื่องมือวิทยาศาสตร์ข้อมูลและเครื่องมืออื่น ๆ ที่ได้รับความนิยมจำนวนมากและมีการติดตั้งและกำหนดค่าไว้ล่วงหน้าเพื่อเริ่มต้นสร้างแอพพลิเคชันอัจฉริยะสำหรับการวิเคราะห์ขั้นสูง ซึ่งเราเลือก Linux edition ของ DSVM บน Ubuntu 16.04 LTS และ OpenLogic 7.2 CentOS-based Linux distributions วัตถุประสงค์ของการใช้งาน DSVM คือการจัดหาเครื่องมือให้ผู้เชี่ยวชาญด้านข้อมูลในทุกระดับและบทบาทด้านทักษะโดยมีสภาพแวดล้อมของข้อมูลทางวิทยาศาสตร์ที่เหมาะสม และช่วยประหยัดเวลาและค่าใช้จ่าย เพราะทุกอย่างพร้อมตั้งแต่ให้เริ่มต้นโครงการข้อมูลทาง Data Science โดยทันทีในอินสแตนซ์ VM ที่สร้างขึ้นใหม่ 

Data Science VM ได้รับการออกแบบและกำหนดค่าสำหรับการทำงานกับช่วงการใช้งานที่หลากหลาย ผู้ใช้งานสามารถปรับขนาดสภาพแวดล้อมของผู้ใช้ขึ้นหรือลงตามที่โครงการของผู้ใช้ต้องการเปลี่ยน และสามารถใช้ภาษาที่ผู้ใช้ต้องการในการเขียนโปรแกรมข้อมูลวิทยาศาสตร์ได้ ผู้ใช้สามารถติดตั้งเครื่องมืออื่นๆ และปรับแต่งระบบตามความต้องการของผู้ใช้ได้

### Power BI
Power BI เป็นชุดเครื่องมือวิเคราะห์ธุรกิจที่ให้ข้อมูลเชิงลึกที่สามารถเชื่อมต่อกับแหล่งข้อมูลนับร้อย ๆ แห่งช่วยลดความซับซ้อนของการจัดเตรียมข้อมูลและผลักดันการวิเคราะห์เฉพาะกิจ สร้างรายงานที่สวยงามจากนั้นเผยแพร่ให้องค์กรของผู้ใช้ สามารถใช้งานได้บนเว็บและบนอุปกรณ์เคลื่อนที่ ทุกคนสามารถสร้างแดชบอร์ดเฉพาะบุคคลโดยมีมุมมองธุรกิจที่ไม่เหมือนใคร 360 องศา และมีขนาดทั่วทั้งองค์กรด้วยการกำกับดูแลและการรักษาความปลอดภัยในตัว
 
Microsoft Power BI นำการวิเคราะห์ขั้นสูงมาสู่ธุรกิจประจำวัน เพิ่มกระบวนการตัดสินใจช่วยให้ผู้ใช้สามารถดึงความรู้ที่มีประโยชน์จากข้อมูลเพื่อแก้ปัญหาทางธุรกิจ ครอบคลุมความสามารถในการวิเคราะห์ขั้นสูงรวมทั้งการคาดการณ์การวิเคราะห์, การสร้างภาพข้อมูล, บูรณาการ R และการวิเคราะห์ข้อมูลการแสดงออก

## ขั้นตอนกระบวนการทำงาน (Workflow scenario)
	เนื่องจากการทำงานในการวิเคราะห์ข้อมูล Big Data เป็นงานที่ซับซ้อน ซึ่งต้องใช้เครื่องมือที่มีประสิทธิภาพและยืดหยุ่นในการทำงานหลายรูปแบบ ดังนั้นในโปรเจ็กนี้เราจะใช้งาน Azure Services สำหรับทุกกระบวนการเพื่อวิเคราะห์ข้อมูลขนาดใหญ่ เพราะ Azure มีเครื่องมือวิเคราะห์ข้อมูลขั้นสูงสำหรับการจัดเก็บข้อมูล การประมวลผลข้อมูล และส่วนประกอบการวิเคราะห์ขั้นสูง ซึ่ง Azure จะช่วยให้ผู้ใช้งานสามารถวิเคราะห์ข้อมูลได้โดยมีความสามารถด้าน Machine Learning เพื่อการ predict  หรือ วิเคราะห์ข้อมูลในรูปแบบต่างๆ ดังนั้นผู้ใช้ต้องวางแผนการดำเนินโดยเริ่มตั้งแต่ การนำเข้าข้อมูลจำนวนมากเข้าสู่ระบบ การเตรียมและเพิ่มคุณภาพ และการ Training และ Testing ทดสอบ จากนั้นจะเป็นการสร้าง model และแสดงผลข้อมูลผลลัพธ์ใน Power BI

ซึ่งในส่วนนี้เป็นการอธิบายการออกแบบกระบวนการทำงานเพื่อให้ผู้ใช้งานสามารถจัดการข้อมูลขนาดใหญ่และการวิเคราะห์ข้อมูลขั้นสูง โดยใช้ประโยชน์จากเทคโนโลยีเหล่านี้ โดยสถาปัตยกรรมประกอบด้วย
- Azure Data Factory (ADF) 
- Azure Data Lake Store 
- HDInsight 
- Azure Machine Learning (Azure ML) 
- Azure Analysis Service
- DSVM
- CosmosDB
- Power BI Desktop 

## โดยขั้นตอนต่างๆ มีดังนี้
เริ่มจาก Ingest เป็นการนำเข้าข้อมูลสู่ระบบ โดยกระบวนการโหลดข้อมูลลงในที่เก็บข้อมูลที่เรียกว่า Data Lake Store โดยใช้เครื่องมือที่ Azure ได้เตรียมให้ซึ่งทำได้หลายวิธี เช่น Command Line, Azure Storage Explorer App, PowerShell, CLI เป็นต้น ขั้นตอนถัดไปเป็นการทำ Data Prep หรือการสำรวจและเตรียมความพร้อมของข้อมูลเช่นการทำ cleaning โดยใช้ Python, Spark SQL ใน HDInsight หรือเครื่องมือใหม่เช่น Azure ML Workbench ต่อจากนั้นผู้ใช้งานสามารถสร้างแบบจำลองโดยใช้ Machine Learning เพื่อทำการวิเคราะห์ข้อมูล สร้าง model และดำเนินการพัฒนา model ให้เป็น Web Service ได้ และเมื่อการวิเคราะห์ข้อมูลเสร็จเรียบร้อย เราจะสามารถแสดงผลของข้อมูล โดยใช้เครื่องมือที่เรียกว่า Power BI ที่เหมาะสำหรับการทำ Data Visualization.
เราพิจารณาใช้ Azure Data Factory (ADF) ซึ่งเป็นเครื่องมือสำหรับการทำ Data Integration เพื่อคัดลอกข้อมูลไปสู่ Azure โดยการสร้าง pipeline ซึ่งประกอบด้วย activities ต่างๆ สำหรับการนำข้อมูลไปจัดทำสำเนาที่กำหนดค่าไว้ การส่งข้อมูลสามารถทำได้ทั้งหมด หรืออาจจะเพียงบางส่วนและยังสามารถกำหนดตารางเวลาในการส่งข้อมูลได้ หรือการส่งข้อมูลแบบต่อเนื่อง ข้อมูลจะถูกจัดเก็บลงในที่เก็บข้อมูลแบบ Azure Data Lake Store ผ่านทาง pipeline ที่กำหนดไว้ และเนื่องจากข้อมูลอยู่ On-Premises ดังนั้นจะมีการติดตั้ง IR (Integration Runtime) เพื่อให้มีการสื่อสารกับ Azure ได้อย่างมีประสิทธิภาพ 
เมื่อข้อมูลได้ถูกจัดเก็บบน Azure Data Lake Store ซึ่งอาจจะอยู่ในรูปแบบต่างๆ เช่น Flat files หลังจากนั้นการประมวลผลข้อมูล Big Data จะเริ่มขึ้นโดย เราสามารถใช้ทั้ง HDInsight หรือ DVSM เนื่องจากทั้งสองให้การสนับสนุน SQL ผ่าน HDFS (Hadoop Distributed File System) หลังจากนั้นนักวิเคราะห์ข้อมูลของ BBL สามารถทำงานเพื่อค้นหาและเตรียมข้อมูลได้ทันที ซึ่งพวกเขาสามารถใช้ Spark SQL พวกเขาสามารถสร้างตารางภายนอกที่เหมาะสมบนไฟล์ ที่เป็นข้อมูล Flat, CSV, TxT หรืออื่นๆ HDFS ซึ่งจะทำงานร่วมกับอินแสตนซ์ HDInsight ของพวกเขาได้ทันที และ HDInsight และ Spark SQL ช่วยให้นักพัฒนาสามารถใช้ Scala หรือ Python เพื่อทำงานกับไฟล์ที่จัดเก็บใน Azure ได้อย่างง่ายดายขณะที่ Spark SQL ช่วยให้สามารถใช้ทักษะ SQL ที่มีอยู่ได้ การใช้ Spark SQL สามารถเขียนข้อมูลลงในตาราง Hive ในที่จัดเก็บได้ ตารางไฮฟ์เหล่านี้สามารถเข้าถึงได้ผ่านทาง Power BI ทำให้สามารถมองเห็นภาพได้ง่ายและสามารถเข้าถึงการรายงานข้อมูลที่ได้จาก Machine Learning.
ในการวิเคราห์ข้อมูล เราจะใช้เทคโนโลยี Machine Learning ที่อยู่บน HDInsight โดยการเขียน Python หรือใช้ Azure Machine Learning ได้ตามความต้องการของ Data Science โดยหลังจากที่ข้อมูลได้ถูกประมวลผลจาก HDInsight แล้วจะอยู่ในรูปแบบ Structure ที่สามารถนำไปใช้ในการวิเคราะห์ได้ โดยสร้างเป็น Data Set และผ่านการทำ Data Preparation ให้เหมาะสมสำหรับการใช้ในการสร้าง model หรืออาจใช้ Spark เพื่อสร้าง subset ของข้อมูลที่ใช้ในการ Training และการตรวจสอบความถูกต้อง ซึ่งทางผู้วิเคราะห์ข้อมูลสามารถเลือก algorithm ต่างๆ เช่น clustering, regression or two-class classification หรืออื่นๆ สำหรับข้อมูลบางอย่างอาจจำเป็นต้องผ่านขั้นตอนการเตรียม เช่น ต้องลบคอลัมน์ออกต้องเปลี่ยนประเภทข้อมูล ขั้นตอนเหล่านี้จะถูกนำมาใช้ในโมเดลได้โดยการทำ Data Prep ข้อมูลสามารถทำได้โดยใช้ภาษา R หรือ Python ซึ่งเป็นภาษาที่คุ้นเคยกับนักวิทยาศาสตร์และนักพัฒนาข้อมูล ภาษาเหล่านี้มีความสามารถในการแปลงข้อมูลที่มีประสิทธิภาพและช่วยให้เกิดความยืดหยุ่นในการล้างข้อมูลในขณะที่ลดความซับซ้อนโดยรวมของโมเดล ML เหล่านี้สามารถแสดงออกได้โดยใช้โมดูล Execute R และ Execute Python Script. Microsoft ยังมีเครื่องมือใหม่ที่อำนวยความสะดวกในการให้ผู้ใช้ทำ Data Prep ผ่านเครื่องมือแบบ GUI ชื่อ Workbench ได้อีกด้วย
สำหรับการวัดความแม่นยำของโมเดล สามารถทำได้โดยการวัดความถูกต้องกับชุดข้อมูล Test model ซึ่งเป็นข้อมูลชุดเล็ก ๆ จะถูกสงวนไว้เป็นตัวอย่างที่จะใช้ในการ "ทดสอบ" แบบจำลองเพื่อดู วิธีดำเนินการกับผลลัพธ์ที่ทราบ ผลลัพธ์ของการคาดเดาและผลลัพธ์ที่ "ถูกต้อง" จะถูกส่งผ่านไปยังโมดูลประเมินผลซึ่งจะให้คะแนนเช่นคะแนนจากแบบจำลองสับสนที่ให้มุมมองต่อความถูกต้องและในสถานการณ์ใดที่โมเดลทำผิดพลาด แบบจำลองการตรวจสอบที่มีความซับซ้อนมากขึ้นอาจเกี่ยวข้องกับการใช้วิธีการรวมกันเช่นแบบจำลอง Cross Validate Model ซึ่งใช้เวลาหลาย ๆ รอบในการป้อนข้อมูลเพื่อยืนยันประสิทธิภาพ
การนำ model ไปใช้งานจริง ทาง BBL สามารถใช้งาน Azure ML Model โดยการใช้ ML Studio เพื่อสร้า Web Service ได้ทันที ซึ่งจะบรรจุฟังก์ชันการให้ Score โดยการเรียกใช้งานผ่าน REST API 
สำหรับการทดสอบ model ก่อนการใช้งานจริงบน Production และการใช้งาน model ที่ซับซ้อนเช่น Deep Learning หรือการทำงานบน GPU นักวิเคราะห์ระบบสามารถเลือกที่จะทำงานบน Azure Data Science VM ในการ Training และ Develop เพราะ DSMV ประกอบไปด้วยเครื่องมือสำคัญชนิดต่างๆ ที่เป็นที่นิยมของนักวิเคราะห์ข้อมูลและยังสามารถหยุดการทำงานของระบบเมื่อไม่มีความจำเป็นต้องใช้ เพื่อเป็นการลดค่าใช้จ่ายอีกด้วย
การแสดงผลของการวิเคราะห์ข้อมูล สามารถใช้งาน Power BI ซึ่งเป็นตัวเลือกที่ดีสำหรับ BBL เพราะ Power BI สามารถทำสิ่งที่เรียกว่า Direct Query ที่ติดต่อโดยตรงกับแหล่งข้อมูล Hive HDInsight Spark รวมถึงมีทางเลือกในการดึงข้อมูลเข้ามาประมวลผลที่ตัวรายงานเอง ทีจะคัดลอกข้อมูลลงในชุดข้อมูลที่ได้รับการจัดการ BI จาก Spark และผู้ใช้งานสามารถแก้ไข เพิ่มเติมข้อมูลโดยใช้คอมโพเนนต์ Query Editor ของแอ็พพลิเคชัน Power BI Desktop จากนั้นพวกเขาสามารถอัปโหลดไฟล์นี้ไปยัง Power BI Service ได้ ด้วยการใช้งาน Power BI พวกเขาสามารถสร้าง Content Pack ที่ประกอบด้วยแดชบอร์ดรายงานและชุดข้อมูลที่ต้องการและ จำกัด การเข้าถึงกลุ่มเหล่านั้นใน Azure Active Directory ที่ควบคุมสิทธิในการเข้าใช้งานตามที่กำหนดได้
สำหรับในกรณีที่มีความจำเป็นในการสร้าง เพิ่มกำลังการผลิตโดยการเพิ่มประสิทธิภาพและทำงานได้เร็วขึ้นเพื่อตอบสนองให้กับลูกค้าได้เร็วขึ้น เราสามารถทำสำเนาข้อมูลเพื่อให้อยู่ใกล้กับผู้ใช้ทั่วโลก Azure Cosmos DB จะใช้ในการกระจาย ฐานข้อมูลไปทั่วโลกซึ่งผู้ใช้ในที่ใกล้ Cosmos DB เข้าถึงได้จากที่ไหนก็ได้ในโลก ในฐานข้อมูลเราทำสำเนาที่มีอยู่ในหลายภูมิภาคทั่วโลกที่ทุกคนสามารถอ่านและเขียนได้และตรวจสอบความตรงกันของชุดข้อมูลอย่างต่อเนื่องเพื่อให้สอดคล้องกันที่สุดเท่าที่จะเป็นไปได้ด้วยความเร็วของแสง Cosmos DB สามารถรองรับการเก็บข้อมูลประเภท NoSQL ได้ถึง 4 รูปแบบประกอบด้วย Key-Value, Document, Column-Family และ Graph.
